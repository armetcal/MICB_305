---
title: "Random Forest"
author: "Avril Metcalfe-Roach"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

This R Markdown file contains code that can produce a publication-worthy
Random Forest. The code will quantify how well a group of predictor
variables (metadata and/or microbiome data) can predict an outcome of
interest.

The following elements are included:

-   Training group and testing group
-   Hyperparameter tuning
-   K-fold cross-validation
-   Feature importance calculation
-   Receiver Operator Characteristic (ROC) curves
-   Area-Under-Curve (AUC) values

# Part 1: Load data

The Moving Pictures dataset is not large enough to run Random Forest
effectively - let's see this for ourselves!

The dataset contains samples from two different people. Can we tell
which samples are from whom?

**If the below code does not run, please 'pull' from GitHub to ensure
that you have the most up-to-date version of the MICB 305 repo.**

```{r Load Libraries and Data}
# install.packages('randomForest')
# install.packages('caret')
# install.packages('ranger')
# install.packages('pROC')
# install.packages('boot')
library(randomForest)
library(caret)
library(ranger)
library(pROC)
library(boot)
library(ggplot2)
library(phyloseq)
library(tidyverse)

# Load the dataset and aggregate reads to the Genus level
ps = readRDS('../Datasets/phyloseq_taxonomy.rds') %>% 
  tax_glom('Genus')
```

# Part 2: Format data

For Random Forest, we need the following elements:

-   Our outcome variable: here we will use subject (subject-1 vs
    subject-2).

-   We should only include taxonomy and variables that we want to use in
    the model.

    -   Not usually ideal to test every single variable - chances of
        overfitting is high
    -   For illustration purposes, we'll just use the top 10 most
        abundant taxa

-   The variables should be normalized. Continuous data (including
    metadata, if applicable) should all be on roughly the same scale.

    -   Microbiome data will [first]{.underline} be CLR-transformed
        (Recall Module 12).

        -   Note: this [must]{.underline} be done before filtering the
            taxa.

    -   These variables will [then]{.underline} be "Z-transformed" (mean
        of zero, standard deviation of 1).

-   The data can't contain missing values. (You may have to filter out
    rows with missing metadata)

```{r Format dataset for RF}
# Calculate average abundance~~~~~~~~~~~~
avg_abundance = taxa_sums(ps)/sum(taxa_sums(ps)) 
# Sort high to low
avg_abundance = sort(avg_abundance, decreasing = T)
# Take the top 10
top_10 = avg_abundance[1:10]
# Extract taxa names 
top_10 = names(top_10)

# Normalize microbiome data and select only the significant taxa
# CLR transformation
ps_clr = ps %>% microbiome::transform('clr') 
# Filter taxa
ps_filt = prune_taxa(top_10,ps_clr)
# Melt the dataset
df = psmelt(ps_filt) %>% 
  # dashes will cause errors
  mutate(subject = str_replace_all(subject,'-','_'))
# Add Z transformation to each Genus individually
# (mean of zero, standard deviation of 1)
df_transformed = df %>% 
  group_by(Genus) %>% 
  mutate(Abundance = scale(Abundance)) %>% 
  ungroup()

# Final table should ONLY contain the outcome and explanatory variables, each as their own column. For now we'll also include the sample id.
df_pivot = df_transformed %>% 
  select(Sample,subject,Genus,Abundance) %>% 
  # Turn each Genus into its own column
  pivot_wider(names_from = Genus, values_from = Abundance)

# Remove rows with NA values in the metadata
df_noNA = df_pivot %>% na.omit()

# Remove the sample ID column - otherwise the code will try to use it as an explanatory variable (just like the microbial genera).
# We do this after pivoting (try doing it before pivoting, see what happens!)
df_final = df_noNA %>% select(-Sample)
```

Now all we have is a data frame with our outcome variable (subject) and
our explanatory variables (the differentially abundant taxa). All
numerical variables are normalized.

```{r}
df_final
```

# Part 3: Set up RF

First, we will separate the data into predictors (taxa) and outcome
(disease status).

```{r predictors_and_outcome}
predictors = df_final %>% select(-subject)

# We will transform subject into a factor.
# The first level of the factor is automatically used as the reference group.
# We will put subject_1 first.
# The results will describe subject 2 relative to the reference (subject_1).
outcome = df_final %>% pull(subject) %>% 
  factor(levels = c('subject_1','subject_2'))
```

Next, we will set up the k-fold cross-validation. We will set k=5, so
that our dataset is randomly broken into 2 groups.

```{r k_fold_CV}
# Randomly subsets the rows into k equal bins.
k = 5
set.seed(421)
folds = createFolds(outcome, k = k, list = TRUE)

# Each of these folds will take a turn being the test dataset.
str(folds)
```

Finally, we will set up our hyperparameter tuning. This is a data frame
that lists a range of possible values for each random forest parameter.
This is what you will edit if your model is improperly tuned.

*FYI: In R, only a few parameters can be modified. As of 2025, python is
the recommended language for running machine learning algorithms, as it
provides the most flexibility (but that is far beyond the scope of our
course!).*

*FYI: there are other ways to run hyperparameter tuning, but this way is
less computationally intensive.*

```{r hyperparameters}
# mtry: number of variables that will be used per forest. 
#       High = overfitting, low = uninformative

# splitrule: affects how decision trees are calculated.
#            Use gini or extratrees for boolean outcomes (ex. subject)
#            Use variance for continuous outcomes (ex. age)

# min.node.size: Related to tree complexity. Larger = simpler tree.
# Often best as a proportional fraction of your sample size.

# These are generic values. Depending on your dataset, you may need to adjust the numeric ranges up or down.
tune_grid = expand.grid(mtry = c(3,6,10), 
                       splitrule = c("gini","extratrees"),
                       min.node.size = c(2,3,4))
```

# Part 4: Run RF

The random forest code is complicated. I have compiled it into a single
function called `run_rf`, which can be loaded using the `source`
command. A helper command will also be loaded.

```{r run_rf}
source('randomforest_functions.R')

pd_model = run_rf(X = predictors, y = outcome, 
                  fold_list = folds,
                  hyper = tune_grid, 
                  rngseed = 421)
```

The pd_model object contains the following:

`auc_train & auc_test:` final AUC values for train/test datasets,
averaged across all k folds

`auc_train_ci & auc_test_ci:` AUC 95% confidence intervals

`test_labels & train_labels:` Predictions for each individual sample

`importance:` Importance values for each predictor (higher = more
important)

```{r inspect_output}
names(pd_model)
```

# Part 5: Interpret Results

First we will generate our ROC curve.

```{r ROC_curve}
# Calculate ROC curve points from RF output
roc_test = roc(pd_model$test_labels$true_labels,
               pd_model$test_labels$predicted_probabilities)
roc_train = roc(pd_model$train_labels$true_labels,
                pd_model$train_labels$predicted_probabilities)

# True positive rate = sensitivity
# False positive rate = 1-specificity
ggplot() +
  # Training data: this is a type of control
  geom_line(aes(x = 1 - roc_train$specificities, 
                y = roc_train$sensitivities), 
            color = "red",size=1) +
  # Test data: tells us the strength of the prediction
  geom_line(aes(x = 1 - roc_test$specificities,
                y = roc_test$sensitivities), 
            color = "black",size=1) +
  geom_abline(slope = 1, intercept = 0, color = "gray", linetype = "dashed",size=1) +
  labs(x = "False Positive Rate", y = "True Positive Rate") +
  annotate("text", x = 0.7, y = 0.2, 
           label = sprintf("Train (red): %.2f (%.2f-%.2f)\nTest (black): %.2f (%.2f-%.2f)",
           auc(roc_train), pd_model$auc_train_ci[1], pd_model$auc_train_ci[2],
           auc(roc_test), pd_model$auc_test_ci[1], pd_model$auc_test_ci[2]), 
           size = 6) +
  theme_minimal(base_size=18)
```

Next, we'll compile the results into a table. This will allow us to
easily compare different models, if desired.

```{r compile_results}
roc_data = data.frame(Dataset = 'RF Tutorial Data',
     Training_AUC = round(pd_model$auc_train, 2),
     Training_AUC_CI = paste0(round(pd_model$auc_train_ci[1], 2), "-", 
                              round(pd_model$auc_train_ci[2], 2)),
     Testing_AUC = round(pd_model$auc_test, 2),
     Testing_AUC_CI = paste0(round(pd_model$auc_test_ci[1], 2), "-", 
                             round(pd_model$auc_test_ci[2], 2)))
```

Finally, we'll plot the importance values of each predictor.

```{r}
pd_model$importance
```

```{r importance_values}
pd_model$importance %>% 
  # Data are automatically arranged by decreasing importance - turn it into a factor.
  # Otherwise the features will show up alphabetically in the plot.
  mutate(Feature = factor(.$Feature,levels = .$Feature)) %>% 
  ggplot(aes(Feature,MeanDecreaseGini,fill=MeanDecreaseGini)) +
  geom_col() +
  theme_classic(base_size=18) +
  theme(axis.text.x = element_text(angle=45, vjust = 1, hjust=1)) +
  ylab('Importance (Gini)') + xlab(NULL)
```

Another sign that it didn't work very well is that the 'most important'
feature doesn't seem to correlate strongly with the subject ID.

How could we improve our results?

```{r Visualize}
temp = ps_clr %>% 
  subset_taxa(Genus=='g__Neisseria') %>% 
  psmelt()

temp %>% ggplot(aes(subject,Abundance)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(height=0, width = 0.2) +
  theme_classic(base_size=18)
```
